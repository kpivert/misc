---
title: "02-resample"
output: html_document
---

```{r setup, include=FALSE}
options(scipen = 999)
library(tidyverse)
library(AmesHousing)
library(tidymodels)

ames <- make_ames() %>% 
  dplyr::select(-matches("Qu"))

set.seed(100)
ames_split <- initial_split(ames)
ames_train <- training(ames_split)
ames_test <- testing(ames_split)
```

# Your Turn 1

Run the first code chunk. Then fill in the blanks to:

1. Create a split object that apportions 75% of `ames` to a training set and the remainder to a testing set.
2. Fit the `all_wf` to the training set.
3. Predict with the testing set and compute the rmse of the fit.

```{r}
rt_spec <- 
  decision_tree() %>% 
  set_engine("rpart") %>% 
  set_mode("regression")

set.seed(100)
```

```{r}
new_split <- ___________(ames)
new_train <- ___________(new_split)
new_test  <- ___________(new_split)

rt_spec %>% 
  fit(Sale_Price ~ ., data = ___________) %>% 
  predict(_______) %>% 
  mutate(truth = _______$Sale_Price) %>% 
  rmse(truth, .pred)
```


# Your Turn 2

What would happen if you repeated this process? Would you get the same answers?

Note your answer from above. Then rerun just the last code chunk above. Do you get the same answer?

Try it a few times.


# Your Turn 3

Run the code below. What does it return?

```{r}
set.seed(100)
cv_folds <- 
    vfold_cv(ames, v = 10, strata = Sale_Price, breaks = 4)
cv_folds
```

# Your Turn 4

Modify the code below to use `fit_resamples` and `cv_folds` to cross-validate the regression tree model. Which RMSE do you collect at the end?

```{r}
set.seed(100)
rt_spec %>% 
  fit(Sale_Price ~ ., data = new_train) %>% 
  predict(new_test) %>% 
  mutate(truth = new_test$Sale_Price) %>% 
  rmse(truth, .pred)
```

